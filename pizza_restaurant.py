# -*- coding: utf-8 -*-
"""Copy of Llama2llamaindexDemo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1874AyQPVNH5LqHAJIbgCpEKBArlR0ED9
"""

# pip install pypdf
# pip install python-dotenv

# pip install -q transformers einops accelerate langchain bitsandbytes

# pip install sentence_transformers

# pip install llama-index

# Generate simulation data
import pandas as pd
import numpy as np
from datetime import timedelta, date

def daterange(start_date, end_date):
    for n in range(int((end_date - start_date).days / 7)):
        yield start_date + timedelta(n * 7)

def generate_data(start_date, end_date):
    data = []

    # Define the base values for the quantities ordered each week
    base_values = [100, 10, 100, 80, 60, 40, 40, 30, 20, 0, 0]
    
    # Define the quantities of each ingredient required per pizza
    pizza_requirements = [0.3, 0.3, 0.2, 0.2]
    
    economic_conditions = ["Good", "Average", "Bad"]
    seasons = ["Winter", "Spring", "Summer", "Fall"]
    events = ["None", "Local_Event1", "Local_Event2", "Superbowl", "Valentine's Day", "St. Patrick's Day", "Easter"]
    
    for single_date in daterange(start_date, end_date):
        month = single_date.month
        season = seasons[0 if month in [12, 1, 2] else 1 if month in [3, 4, 5] else 2 if month in [6, 7, 8] else 3]
        economic_condition = np.random.choice(economic_conditions, p=[0.2, 0.6, 0.2])
        
        row = [single_date.strftime("%Y-%m-%d"), season, economic_condition]
        
        event = "None"
        if month == 2 and 1 < single_date.day < 15:
            event = "Valentine's Day"
        elif month == 3 and 15 < single_date.day < 23:
            event = "St. Patrick's Day"
        elif month == 4 and single_date.weekday() == 6 and 15 < single_date.day < 23:
            event = "Easter"
        row.append(event)
        
        adjustment_factor = 1.0 + 0.2 * (economic_condition == "Good") - 0.2 * (economic_condition == "Bad")
        seasonal_adjustment = 1.0 + 0.2 * (season == "Summer") - 0.1 * (season == "Winter")
        
        ordered_quantities = []
        for base_value in base_values:
            value = base_value * adjustment_factor * seasonal_adjustment
            value += value * np.random.uniform(-0.1, 0.1) # Adding some random noise
            ordered_quantities.append(round(value, 2))
        
        # Estimate the number of pizzas sold as the minimum of the ratios of the quantities ordered to the quantity required per pizza
        pizzas_sold = min(ordered_quantities[i] / req for i, req in enumerate(pizza_requirements) if req > 0)
        row.append(round(pizzas_sold))
        
        row.extend(ordered_quantities)
        
        data.append(row)
    return data

# Define column names
# Update column names to include "Pizzas_Sold"
columns = ["Date", "Season", "Economic_condition", "Event", "Pizzas_Sold", "Regular_flour_kg", "Gluten_free_flour_kg", "Tomato_sauce_liters", "Mozzarella_cheese_kg", 
           "Pepperoni_kg", "Bell_peppers_kg", "Mushrooms_kg", "Olives_kg", "Pineapple_kg", "Specialty_pizza_ingredient1_kg", "Specialty_pizza_ingredient2_kg"]

# Generate data from 2023 to 2033
start_date = date(2010, 1, 1)
end_date = date(2023, 12, 31)
data = generate_data(start_date, end_date)

# Create a DataFrame from the data
df = pd.DataFrame(data, columns=columns)

# Save the data to a CSV file
df.to_csv('pizza_restaurant_data.csv', index=False)

print("Data has been generated and saved to 'pizza_restaurant_data.csv'")

df = pd.read_csv('pizza_restaurant_data.csv')
df['year'] = pd.to_datetime(df['Date']).dt.year

test = df[(df['Season'] == 'Summer') & (df['year'] == 2020)]
test['Pizzas_Sold'].sum()

import logging
import os
import sys
import torch

# Configure environment variables
# Setting the maximum number of threads that NumExpr can use to 56 to potentially improve performance
os.environ['NUMEXPR_MAX_THREADS'] = '56'

# Set up logging
# Configuring the basic settings for logging, setting the logging level to INFO, 
# which means that INFO and higher level messages will be logged
logging.basicConfig(stream=sys.stdout, level=logging.INFO)

# Adding a stream handler that outputs log messages to sys.stdout
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

# Import necessary classes and functions
# Importing classes and functions needed to initialize the components in the next sections
from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, LangchainEmbedding
from llama_index.llms import HuggingFaceLLM
from llama_index.prompts.prompts import SimpleInputPrompt
from langchain.embeddings.huggingface import HuggingFaceEmbeddings

# Initialize necessary components
# Defining a prompt that will guide the assistant's behavior while answering questions
system_prompt = "You are a manager at a pizza restaurant and are in charge of ordering supplies for the week based on previous weeks and any seasonal adjustments. You are also need to handle any upcoming parties or events that will require more supplies."

# Creating a wrapper for the query using a simple input prompt
query_wrapper_prompt = SimpleInputPrompt("{query_str}")

# Loading documents from the "data/" directory using SimpleDirectoryReader
documents = SimpleDirectoryReader("data/pizza/").load_data()

# Initializing the embedding model with a pretrained HuggingFace model
embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2"))

# Logging in to the Hugging Face CLI - this is a command line instruction and should be run in a CLI, not in a Python script
# huggingface-cli login

# Configure LLM
# Initializing a HuggingFace LLM (Language model) with various settings such as context window size, token limits, and others
llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={
        "do_sample": True,
        "temperature": 1.0,
        "top_p": 0.9,
    },
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name="meta-llama/Llama-2-7b-chat-hf",
    model_name="meta-llama/Llama-2-7b-chat-hf",
    device_map="auto",
)

# Configure service context and index
# Creating a service context with default settings, including the chunk size and the LLM and embedding model initialized earlier
service_context = ServiceContext.from_defaults(
    chunk_size=1024,
    llm=llm,
    embed_model=embed_model
)

# Creating an index from the loaded documents with the service context
index = VectorStoreIndex.from_documents(documents, service_context=service_context)

# Creating a query engine from the index to facilitate querying
query_engine = index.as_query_engine()

# Define a function to ask questions
def ask_question(question_str):
    return query_engine.query(question_str)





q1 = ask_question("How many pizzas were sold during summer of 2020?")
print(q1)


q2 = ask_question("It is Sept 2024 and I have a big event coming up with 100 people. Can you please provide exact supply details to feed these people? Also, how many pizzas will this make?")
print(q2)
